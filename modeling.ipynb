{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "punctuations = string.punctuation\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_one_doc(text):\n",
    "    \"\"\"\n",
    "    normalize texts in one doc\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokens = [word for word in word_tokenize(text) if word.isalpha()]\n",
    "        tokens = list(filter(lambda t: t not in punctuations, tokens))\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop_words, tokens))\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.search(\"[a-zA-Z]\", token):\n",
    "                filtered_tokens.append(token)\n",
    "        filtered_tokens = list(map(lambda token: wnl.lemmatize(token.lower()), filtered_tokens))\n",
    "        filtered_tokens = list(filter(lambda t: t not in punctuations, filtered_tokens))\n",
    "        return filtered_tokens\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_docs(df, save=True):\n",
    "    \"\"\"\n",
    "    apply text normalization to all docs\n",
    "    \"\"\"\n",
    "    df[\"cleaned_text\"] = df[\"text\"].map(normalize_one_doc)\n",
    "    df[\"text_len\"] = df[\"cleaned_text\"].map(lambda x: len(x))\n",
    "    \n",
    "    if save:\n",
    "        df.to_pickle(\"normalized_data.pkl\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file=\"glove.6B.100d.txt\"):\n",
    "    \"\"\"\n",
    "    load pre-train word embedding\n",
    "    \"\"\"\n",
    "    f = open(file)\n",
    "    embed_index = {}\n",
    "    for line in f:\n",
    "        val = line.split()\n",
    "        word = val[0]\n",
    "        coff = np.asarray(val[1:], dtype =\"float\")\n",
    "        embed_index[word] = coff\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embed_index))\n",
    "    return embed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(docs, embed_index, embed_dim=EMBEDDING_DIM, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    apply Glove 6b embedding vectors to word sequences\n",
    "    \"\"\"\n",
    "    X = np.zeros((len(docs), max_len, 100))\n",
    "    for i, item in enumerate(docs):\n",
    "        for j, word in enumerate(item):\n",
    "            if j < max_len:\n",
    "                temp = embed_index.get(word)\n",
    "                if temp is not None :\n",
    "                    X[i, j, :] = temp\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modeling_data(df):\n",
    "    \"\"\"\n",
    "    wrapper function that prepares data ready for training neural network\n",
    "    \"\"\"\n",
    "    # get response and features\n",
    "    y = df[\"normalized_change\"].apply(lambda x: 1 if x >= 0 else 0)    \n",
    "    docs = df['cleaned_text']\n",
    "    \n",
    "    # train and test split\n",
    "    y_train, y_test, docs_train, docs_test = train_test_split(y, docs, stratify=y, test_size=0.3, random_state=20)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    docs_train.reset_index(drop=True, inplace=True)\n",
    "    docs_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # handling imbalanced labels\n",
    "    y_train, y_test, docs_train, docs_test = y_train.values, y_test.values, docs_train.values, docs_test.values\n",
    "    \n",
    "    # apply Glove word vectors\n",
    "    embed_index = load_embeddings()        \n",
    "    docs_train, docs_test = word2vec(docs_train, embed_index), word2vec(docs_test, embed_index)\n",
    "    \n",
    "    return y_train, y_test, docs_train, docs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(option=\"LSTM\"):\n",
    "    \"\"\"\n",
    "    build neural network\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    if option == \"LSTM\":\n",
    "        model.add(LSTM(32, return_sequences=True, input_shape=(MAX_LEN, EMBEDDING_DIM)))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    elif option == \"BiLSTM\":\n",
    "        model.add(Bidirectional(LSTM(32, input_shape=(MAX_LEN, EMBEDDING_DIM))))    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    evaluate model given truth\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X).flatten()\n",
    "    acc = ((1 * (y_pred>=0.5)) == y).mean()\n",
    "    auc = roc_auc_score(y, y_pred)\n",
    "    print(\"Accuracy: {}; AUC: {}\".format(acc, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_pickle(\"doc_and_financial_data.pkl\")\n",
    "    \n",
    "    # Step 1. Clean up docs and save each doc as a list of lemmatized words\n",
    "    df = normalize_docs(df)\n",
    "    \n",
    "    # Step 2. Prepare data for modeling\n",
    "    y_train, y_test, docs_train, docs_test = create_modeling_data(df)\n",
    "    \n",
    "    # Step 3. Training neural network\n",
    "    model_lstm = build_model(\"LSTM\")\n",
    "    model_lstm.fit(docs_train, y_train, epochs=5, batch_size=64)\n",
    "    evaluate_model(model_lstm, docs_test, y_test)\n",
    "    \n",
    "    model_bilstm = build_model(\"BiLSTM\")\n",
    "    model_bilstm.fit(docs_train, y_train, epochs=5, batch_size=64)\n",
    "    evaluate_model(model_bilstm, docs_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
