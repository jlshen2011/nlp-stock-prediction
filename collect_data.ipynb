{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "from config import Config\n",
    "import datetime as dt\n",
    "import dateutil.relativedelta\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas_datareader.data as web\n",
    "import pandas_market_calendars as mcal\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYSE_HOLIDAYS = mcal.get_calendar(\"NYSE\").holidays().holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ciks(tickers=None):\n",
    "    \"\"\"\n",
    "    get CIK given a company ticker that are needed to download SEC does\n",
    "    \"\"\"\n",
    "    wiki_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    df = pd.read_html(wiki_url, header=0, index_col=0)[0]\n",
    "    if tickers:\n",
    "        df = df.loc[df.index.isin(tickers)]\n",
    "    df = df.reset_index()\n",
    "    res = {ticker: cik for ticker, cik in zip(df[\"Symbol\"], df[\"CIK\"])}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_8k_links(ticker2cik, rundate):\n",
    "    \"\"\"\n",
    "    get links to 8K docs given company CIKs and tickers\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for ticker, cik in ticker2cik.items():\n",
    "        try:\n",
    "            base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "            payload = {\n",
    "                \"action\": \"getcompany\",\n",
    "                \"CIK\": cik,\n",
    "                \"type\": \"8-K\",\n",
    "                \"output\": \"xml\",\n",
    "                \"dateb\": rundate.strftime(\"%Y%m%d\")\n",
    "            }\n",
    "            sec_response = requests.get(url=base_url, params=payload)\n",
    "            soup = BeautifulSoup(sec_response.text, \"lxml\")\n",
    "            url_list = soup.findAll(\"filinghref\")\n",
    "            html_list = []\n",
    "            \n",
    "            # get html version of links\n",
    "            for link in url_list:\n",
    "                link = link.string\n",
    "                if link.split(\".\")[len(link.split(\".\")) - 1] == \"htm\":\n",
    "                    txtlink = link + \"l\"\n",
    "                    html_list.append(txtlink)\n",
    "\n",
    "            doc_list = []\n",
    "            doc_name_list = []\n",
    "            \n",
    "            # get links for txt versions of files\n",
    "            for k in range(len(html_list)):\n",
    "                txt_doc = html_list[k].replace(\"-index.html\", \".txt\")\n",
    "                doc_name = txt_doc.split(\"/\")[-1]\n",
    "                doc_list.append(txt_doc)\n",
    "                doc_name_list.append(doc_name)\n",
    "                \n",
    "            # create dataframe of CIK, doc name, and txt link\n",
    "            df_link = pd.DataFrame({\n",
    "                \"cik\": [cik] * len(html_list),\n",
    "                \"ticker\": [ticker] * len(html_list),\n",
    "                \"txt_link\": doc_list,\n",
    "                \"doc_name\": doc_name_list\n",
    "            })\n",
    "            df_list.append(df_link)\n",
    "        except:\n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"Get {} links for {} companies!\".format(str(len(df)), str(len(tickers) - 1)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_8k_doc(link):\n",
    "    \"\"\"\n",
    "    download a single doc given the link\n",
    "    \"\"\"\n",
    "    r = requests.get(link)\n",
    "    filing = BeautifulSoup(r.content, \"html5lib\", from_encoding=\"ascii\")\n",
    "    submission_dt = filing.find(\"acceptance-datetime\").string[:14]            \n",
    "    submission_dt = dt.datetime.strptime(submission_dt, \"%Y%m%d%H%M%S\")\n",
    "    for section in filing.findAll(\"html\"):\n",
    "        try:\n",
    "            # remove tables\n",
    "            for table in section(\"table\"):\n",
    "                table.decompose()\n",
    "            # convert to unicode\n",
    "            section = unicodedata.normalize(\"NFKD\",section.text)\n",
    "            section = section.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"/s\", \" \").replace(\"\\'\", \"'\")            \n",
    "        except AttributeError:\n",
    "            section = str(section.encode(\"utf-8\"))\n",
    "    filing = \"\".join((section))\n",
    "    return filing, submission_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_8k_docs(tickers, by_date, save=True):\n",
    "    \"\"\"\n",
    "    wraper function that downloads docs given tickers\n",
    "    \"\"\"\n",
    "    # step 1: get CIK given tickers\n",
    "    ticker2cik = get_ciks(tickers)\n",
    "\n",
    "    # step 2: get doc links given CIKs and tickers\n",
    "    df = get_8k_links(ticker2cik, by_date)\n",
    "\n",
    "    # step 3: get docs given links\n",
    "    df[\"text\"], df[\"release_date\"] = \"\", dt.date(1900, 1, 1)\n",
    "    success_count, failure_count = 0, 0\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            df.loc[df.index[i], \"text\"], df.loc[df.index[i], \"release_date\"] = get_one_8k_doc(df.loc[df.index[i], \"txt_link\"])\n",
    "            success_count += 1\n",
    "        except:\n",
    "            failure_count += 1\n",
    "        if i % 10 == 9:\n",
    "            gc.collect()\n",
    "            print(\"Downloading {} docs. {} succeeded, {} failed ...\".format(i + 1, success_count, failure_count))\n",
    "    print(\"Downloading {} docs. {} succeeded, {} failed ...\".format(i + 1, success_count, failure_count))\n",
    "    \n",
    "    if save:\n",
    "        df.to_pickle(\"doc_data.pkl\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financial_data(tickers, start_date, end_date, save=True):    \n",
    "    \"\"\"\n",
    "    get daily stock change data\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    for ticker in tickers:        \n",
    "        try:\n",
    "            data = web.DataReader(ticker, \"yahoo\", start=str(start_date), end=str(end_date))\n",
    "            data[\"ticker\"] = ticker\n",
    "            df.append(data)\n",
    "            print(\"Downloaded daily price data for {}.\".format(ticker))\n",
    "        except:\n",
    "            print(\"Failed to download daily price data for {}.\".format(ticker))            \n",
    "    df = pd.concat(df)\n",
    "    df = df.reset_index()\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    df[\"date\"] = df[\"date\"].apply(lambda x: x.date())\n",
    "    \n",
    "    if save:\n",
    "        df.to_pickle(\"financial_data.pkl\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekday_check(date, incremental):\n",
    "    \"\"\"\n",
    "    move a date to next \"good\" date if it's weekend/holiday\n",
    "    \"\"\"\n",
    "    while date.isoweekday() > 5 or date.date() in NYSE_HOLIDAYS:\n",
    "        date = date + dt.timedelta(days=incremental)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pct_change(start_price, end_price):\n",
    "    \"\"\"\n",
    "    calculate percent change given two prices\n",
    "    \"\"\"\n",
    "    pct_change = (end_price - start_price) / start_price\n",
    "    pct_change = round(pct_change, 4) * 100\n",
    "    return pct_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df_docs, df_financial, save=True):\n",
    "    \"\"\"\n",
    "    for each doc/row, compute the corresponding normalized price change\n",
    "    \"\"\"\n",
    "    df_docs[\"price_pct_change\"] = 0\n",
    "    df_docs[\"index_pct_change\"] = 0\n",
    "    df_docs[\"normalized_change\"] = 0\n",
    "    df_docs[\"start_date\"] = dt.date(1900, 1, 1)\n",
    "    df_docs[\"end_date\"] = dt.date(1900, 1, 1)\n",
    "    \n",
    "    for i in range(len(df_docs)):\n",
    "        try:\n",
    "            row = df_docs.loc[i]\n",
    "            ticker, release_date = row[\"ticker\"], row[\"release_date\"]\n",
    "            if release_date > dt.datetime(1900, 1, 1):\n",
    "                market_close = release_date.replace(hour=16, minute=0, second=0)\n",
    "                market_open = release_date.replace(hour=9, minute=30, second=0)\n",
    "\n",
    "                # if report is released after market hours, take change of start date close and release date open        \n",
    "                if release_date > market_close:\n",
    "                    start_date = release_date\n",
    "                    end_date = release_date + dt.timedelta(days=1)\n",
    "                    end_date = weekday_check(end_date, 1)\n",
    "                    start_date, end_date = start_date.date(), end_date.date()                \n",
    "                    start_price_col, end_price_col = \"close\", \"open\"    \n",
    "\n",
    "                # if report is released before market hours, take change of start date's close and release date's open        \n",
    "                elif release_date < market_open:\n",
    "                    start_date = release_date + dt.timedelta(days=-1)\n",
    "                    start_date = weekday_check(start_date, -1)\n",
    "                    end_date = release_date\n",
    "                    start_date, end_date = start_date.date(), end_date.date()\n",
    "                    start_price_col, end_price_col = \"close\", \"open\"     \n",
    "\n",
    "                # if report is released during market hours, use market close        \n",
    "                else:\n",
    "                    start_date = release_date\n",
    "                    end_date = release_date  \n",
    "                    start_date, end_date = start_date.date(), end_date.date()                \n",
    "                    start_price_col, end_price_col = \"open\", \"close\"       \n",
    "\n",
    "                start_price = df_financial.loc[(df_financial[\"ticker\"] == ticker) & (df_financial[\"date\"] == start_date), start_price_col].values[0]\n",
    "                end_price = df_financial.loc[(df_financial[\"ticker\"] == ticker) & (df_financial[\"date\"] == end_date), end_price_col].values[0]\n",
    "                index_start_price = df_financial.loc[(df_financial[\"ticker\"] == \"^GSPC\") & (df_financial[\"date\"] == start_date), start_price_col].values[0]\n",
    "                index_end_price = df_financial.loc[(df_financial[\"ticker\"] == \"^GSPC\") & (df_financial[\"date\"] == end_date), end_price_col].values[0]\n",
    "\n",
    "                price_pct_change = calculate_pct_change(start_price, end_price)\n",
    "                index_pct_change = calculate_pct_change(index_start_price, index_end_price)\n",
    "                normalized_change = price_pct_change - index_pct_change\n",
    "                \n",
    "                df_docs.loc[i, \"price_pct_change\"] = price_pct_change\n",
    "                df_docs.loc[i, \"index_pct_change\"] = index_pct_change\n",
    "                df_docs.loc[i, \"normalized_change\"] = normalized_change\n",
    "                df_docs.loc[i, \"start_date\"] = start_date\n",
    "                df_docs.loc[i, \"end_date\"] = end_date\n",
    "                                        \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "    if save:\n",
    "        df_docs.to_pickle(\"doc_and_financial_data.pkl\")\n",
    "    \n",
    "    return df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 1892 links for 50 companies!\n",
      "Downloading 10 docs. 10 succeeded, 0 failed ...\n",
      "Downloading 20 docs. 20 succeeded, 0 failed ...\n",
      "Downloading 30 docs. 30 succeeded, 0 failed ...\n",
      "Downloading 40 docs. 40 succeeded, 0 failed ...\n",
      "Downloading 50 docs. 50 succeeded, 0 failed ...\n",
      "Downloading 60 docs. 60 succeeded, 0 failed ...\n",
      "Downloading 70 docs. 70 succeeded, 0 failed ...\n",
      "Downloading 80 docs. 80 succeeded, 0 failed ...\n",
      "Downloading 90 docs. 90 succeeded, 0 failed ...\n",
      "Downloading 100 docs. 100 succeeded, 0 failed ...\n",
      "Downloading 110 docs. 110 succeeded, 0 failed ...\n",
      "Downloading 120 docs. 120 succeeded, 0 failed ...\n",
      "Downloading 130 docs. 130 succeeded, 0 failed ...\n",
      "Downloading 140 docs. 140 succeeded, 0 failed ...\n",
      "Downloading 150 docs. 150 succeeded, 0 failed ...\n",
      "Downloading 160 docs. 160 succeeded, 0 failed ...\n",
      "Downloading 170 docs. 170 succeeded, 0 failed ...\n",
      "Downloading 180 docs. 180 succeeded, 0 failed ...\n",
      "Downloading 190 docs. 190 succeeded, 0 failed ...\n",
      "Downloading 200 docs. 200 succeeded, 0 failed ...\n",
      "Downloading 210 docs. 210 succeeded, 0 failed ...\n",
      "Downloading 220 docs. 220 succeeded, 0 failed ...\n",
      "Downloading 230 docs. 230 succeeded, 0 failed ...\n",
      "Downloading 240 docs. 240 succeeded, 0 failed ...\n",
      "Downloading 250 docs. 250 succeeded, 0 failed ...\n",
      "Downloading 260 docs. 260 succeeded, 0 failed ...\n",
      "Downloading 270 docs. 270 succeeded, 0 failed ...\n",
      "Downloading 280 docs. 280 succeeded, 0 failed ...\n",
      "Downloading 290 docs. 290 succeeded, 0 failed ...\n",
      "Downloading 300 docs. 300 succeeded, 0 failed ...\n",
      "Downloading 310 docs. 310 succeeded, 0 failed ...\n",
      "Downloading 320 docs. 320 succeeded, 0 failed ...\n",
      "Downloading 330 docs. 330 succeeded, 0 failed ...\n",
      "Downloading 340 docs. 340 succeeded, 0 failed ...\n",
      "Downloading 350 docs. 350 succeeded, 0 failed ...\n",
      "Downloading 360 docs. 360 succeeded, 0 failed ...\n",
      "Downloading 370 docs. 370 succeeded, 0 failed ...\n",
      "Downloading 380 docs. 380 succeeded, 0 failed ...\n",
      "Downloading 390 docs. 390 succeeded, 0 failed ...\n",
      "Downloading 400 docs. 400 succeeded, 0 failed ...\n",
      "Downloading 410 docs. 410 succeeded, 0 failed ...\n",
      "Downloading 420 docs. 420 succeeded, 0 failed ...\n",
      "Downloading 430 docs. 430 succeeded, 0 failed ...\n",
      "Downloading 440 docs. 440 succeeded, 0 failed ...\n",
      "Downloading 450 docs. 450 succeeded, 0 failed ...\n",
      "Downloading 460 docs. 460 succeeded, 0 failed ...\n",
      "Downloading 470 docs. 470 succeeded, 0 failed ...\n",
      "Downloading 480 docs. 480 succeeded, 0 failed ...\n",
      "Downloading 490 docs. 490 succeeded, 0 failed ...\n",
      "Downloading 500 docs. 500 succeeded, 0 failed ...\n",
      "Downloading 510 docs. 510 succeeded, 0 failed ...\n",
      "Downloading 520 docs. 520 succeeded, 0 failed ...\n",
      "Downloading 530 docs. 530 succeeded, 0 failed ...\n",
      "Downloading 540 docs. 540 succeeded, 0 failed ...\n",
      "Downloading 550 docs. 550 succeeded, 0 failed ...\n",
      "Downloading 560 docs. 560 succeeded, 0 failed ...\n",
      "Downloading 570 docs. 570 succeeded, 0 failed ...\n",
      "Downloading 580 docs. 580 succeeded, 0 failed ...\n",
      "Downloading 590 docs. 590 succeeded, 0 failed ...\n",
      "Downloading 600 docs. 600 succeeded, 0 failed ...\n",
      "Downloading 610 docs. 610 succeeded, 0 failed ...\n",
      "Downloading 620 docs. 620 succeeded, 0 failed ...\n",
      "Downloading 630 docs. 630 succeeded, 0 failed ...\n",
      "Downloading 640 docs. 640 succeeded, 0 failed ...\n",
      "Downloading 650 docs. 650 succeeded, 0 failed ...\n",
      "Downloading 660 docs. 660 succeeded, 0 failed ...\n",
      "Downloading 670 docs. 670 succeeded, 0 failed ...\n",
      "Downloading 680 docs. 680 succeeded, 0 failed ...\n",
      "Downloading 690 docs. 690 succeeded, 0 failed ...\n",
      "Downloading 700 docs. 700 succeeded, 0 failed ...\n",
      "Downloading 710 docs. 710 succeeded, 0 failed ...\n",
      "Downloading 720 docs. 720 succeeded, 0 failed ...\n",
      "Downloading 730 docs. 730 succeeded, 0 failed ...\n",
      "Downloading 740 docs. 740 succeeded, 0 failed ...\n",
      "Downloading 750 docs. 750 succeeded, 0 failed ...\n",
      "Downloading 760 docs. 760 succeeded, 0 failed ...\n",
      "Downloading 770 docs. 770 succeeded, 0 failed ...\n",
      "Downloading 780 docs. 780 succeeded, 0 failed ...\n",
      "Downloading 790 docs. 790 succeeded, 0 failed ...\n",
      "Downloading 800 docs. 800 succeeded, 0 failed ...\n",
      "Downloading 810 docs. 810 succeeded, 0 failed ...\n",
      "Downloading 820 docs. 820 succeeded, 0 failed ...\n",
      "Downloading 830 docs. 830 succeeded, 0 failed ...\n",
      "Downloading 840 docs. 840 succeeded, 0 failed ...\n",
      "Downloading 850 docs. 850 succeeded, 0 failed ...\n",
      "Downloading 860 docs. 860 succeeded, 0 failed ...\n",
      "Downloading 870 docs. 870 succeeded, 0 failed ...\n",
      "Downloading 880 docs. 880 succeeded, 0 failed ...\n",
      "Downloading 890 docs. 890 succeeded, 0 failed ...\n",
      "Downloading 900 docs. 900 succeeded, 0 failed ...\n",
      "Downloading 910 docs. 910 succeeded, 0 failed ...\n",
      "Downloading 920 docs. 920 succeeded, 0 failed ...\n",
      "Downloading 930 docs. 930 succeeded, 0 failed ...\n",
      "Downloading 940 docs. 940 succeeded, 0 failed ...\n",
      "Downloading 950 docs. 950 succeeded, 0 failed ...\n",
      "Downloading 960 docs. 960 succeeded, 0 failed ...\n",
      "Downloading 970 docs. 970 succeeded, 0 failed ...\n",
      "Downloading 980 docs. 980 succeeded, 0 failed ...\n",
      "Downloading 990 docs. 990 succeeded, 0 failed ...\n",
      "Downloading 1000 docs. 1000 succeeded, 0 failed ...\n",
      "Downloading 1010 docs. 1010 succeeded, 0 failed ...\n",
      "Downloading 1020 docs. 1020 succeeded, 0 failed ...\n",
      "Downloading 1030 docs. 1030 succeeded, 0 failed ...\n",
      "Downloading 1040 docs. 1040 succeeded, 0 failed ...\n",
      "Downloading 1050 docs. 1050 succeeded, 0 failed ...\n",
      "Downloading 1060 docs. 1060 succeeded, 0 failed ...\n",
      "Downloading 1070 docs. 1070 succeeded, 0 failed ...\n",
      "Downloading 1080 docs. 1080 succeeded, 0 failed ...\n",
      "Downloading 1090 docs. 1090 succeeded, 0 failed ...\n",
      "Downloading 1100 docs. 1100 succeeded, 0 failed ...\n",
      "Downloading 1110 docs. 1110 succeeded, 0 failed ...\n",
      "Downloading 1120 docs. 1120 succeeded, 0 failed ...\n",
      "Downloading 1130 docs. 1130 succeeded, 0 failed ...\n",
      "Downloading 1140 docs. 1140 succeeded, 0 failed ...\n",
      "Downloading 1150 docs. 1150 succeeded, 0 failed ...\n",
      "Downloading 1160 docs. 1160 succeeded, 0 failed ...\n",
      "Downloading 1170 docs. 1170 succeeded, 0 failed ...\n",
      "Downloading 1180 docs. 1180 succeeded, 0 failed ...\n",
      "Downloading 1190 docs. 1190 succeeded, 0 failed ...\n",
      "Downloading 1200 docs. 1200 succeeded, 0 failed ...\n",
      "Downloading 1210 docs. 1210 succeeded, 0 failed ...\n",
      "Downloading 1220 docs. 1220 succeeded, 0 failed ...\n",
      "Downloading 1230 docs. 1230 succeeded, 0 failed ...\n",
      "Downloading 1240 docs. 1240 succeeded, 0 failed ...\n",
      "Downloading 1250 docs. 1250 succeeded, 0 failed ...\n",
      "Downloading 1260 docs. 1260 succeeded, 0 failed ...\n",
      "Downloading 1270 docs. 1270 succeeded, 0 failed ...\n",
      "Downloading 1280 docs. 1280 succeeded, 0 failed ...\n",
      "Downloading 1290 docs. 1290 succeeded, 0 failed ...\n",
      "Downloading 1300 docs. 1300 succeeded, 0 failed ...\n",
      "Downloading 1310 docs. 1310 succeeded, 0 failed ...\n",
      "Downloading 1320 docs. 1320 succeeded, 0 failed ...\n",
      "Downloading 1330 docs. 1330 succeeded, 0 failed ...\n",
      "Downloading 1340 docs. 1340 succeeded, 0 failed ...\n",
      "Downloading 1350 docs. 1350 succeeded, 0 failed ...\n",
      "Downloading 1360 docs. 1360 succeeded, 0 failed ...\n",
      "Downloading 1370 docs. 1370 succeeded, 0 failed ...\n",
      "Downloading 1380 docs. 1380 succeeded, 0 failed ...\n",
      "Downloading 1390 docs. 1390 succeeded, 0 failed ...\n",
      "Downloading 1400 docs. 1400 succeeded, 0 failed ...\n",
      "Downloading 1410 docs. 1410 succeeded, 0 failed ...\n",
      "Downloading 1420 docs. 1420 succeeded, 0 failed ...\n",
      "Downloading 1430 docs. 1430 succeeded, 0 failed ...\n",
      "Downloading 1440 docs. 1440 succeeded, 0 failed ...\n",
      "Downloading 1450 docs. 1450 succeeded, 0 failed ...\n",
      "Downloading 1460 docs. 1460 succeeded, 0 failed ...\n",
      "Downloading 1470 docs. 1470 succeeded, 0 failed ...\n",
      "Downloading 1480 docs. 1480 succeeded, 0 failed ...\n",
      "Downloading 1490 docs. 1490 succeeded, 0 failed ...\n",
      "Downloading 1500 docs. 1500 succeeded, 0 failed ...\n",
      "Downloading 1510 docs. 1510 succeeded, 0 failed ...\n",
      "Downloading 1520 docs. 1520 succeeded, 0 failed ...\n",
      "Downloading 1530 docs. 1530 succeeded, 0 failed ...\n",
      "Downloading 1540 docs. 1540 succeeded, 0 failed ...\n",
      "Downloading 1550 docs. 1550 succeeded, 0 failed ...\n",
      "Downloading 1560 docs. 1560 succeeded, 0 failed ...\n",
      "Downloading 1570 docs. 1570 succeeded, 0 failed ...\n",
      "Downloading 1580 docs. 1580 succeeded, 0 failed ...\n",
      "Downloading 1590 docs. 1590 succeeded, 0 failed ...\n",
      "Downloading 1600 docs. 1600 succeeded, 0 failed ...\n",
      "Downloading 1610 docs. 1610 succeeded, 0 failed ...\n",
      "Downloading 1620 docs. 1620 succeeded, 0 failed ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1630 docs. 1630 succeeded, 0 failed ...\n",
      "Downloading 1640 docs. 1640 succeeded, 0 failed ...\n",
      "Downloading 1650 docs. 1650 succeeded, 0 failed ...\n",
      "Downloading 1660 docs. 1660 succeeded, 0 failed ...\n",
      "Downloading 1670 docs. 1670 succeeded, 0 failed ...\n",
      "Downloading 1680 docs. 1680 succeeded, 0 failed ...\n",
      "Downloading 1690 docs. 1690 succeeded, 0 failed ...\n",
      "Downloading 1700 docs. 1700 succeeded, 0 failed ...\n",
      "Downloading 1710 docs. 1710 succeeded, 0 failed ...\n",
      "Downloading 1720 docs. 1720 succeeded, 0 failed ...\n",
      "Downloading 1730 docs. 1730 succeeded, 0 failed ...\n",
      "Downloading 1740 docs. 1740 succeeded, 0 failed ...\n",
      "Downloading 1750 docs. 1750 succeeded, 0 failed ...\n",
      "Downloading 1760 docs. 1760 succeeded, 0 failed ...\n",
      "Downloading 1770 docs. 1770 succeeded, 0 failed ...\n",
      "Downloading 1780 docs. 1780 succeeded, 0 failed ...\n",
      "Downloading 1790 docs. 1790 succeeded, 0 failed ...\n",
      "Downloading 1800 docs. 1800 succeeded, 0 failed ...\n",
      "Downloading 1810 docs. 1810 succeeded, 0 failed ...\n",
      "Downloading 1820 docs. 1820 succeeded, 0 failed ...\n",
      "Downloading 1830 docs. 1830 succeeded, 0 failed ...\n",
      "Downloading 1840 docs. 1840 succeeded, 0 failed ...\n",
      "Downloading 1850 docs. 1850 succeeded, 0 failed ...\n",
      "Downloading 1860 docs. 1860 succeeded, 0 failed ...\n",
      "Downloading 1870 docs. 1870 succeeded, 0 failed ...\n",
      "Downloading 1880 docs. 1880 succeeded, 0 failed ...\n",
      "Downloading 1890 docs. 1890 succeeded, 0 failed ...\n",
      "Downloading 1892 docs. 1892 succeeded, 0 failed ...\n",
      "Downloaded daily price data for MSFT.\n",
      "Downloaded daily price data for AAPL.\n",
      "Downloaded daily price data for AMZN.\n",
      "Downloaded daily price data for FB.\n",
      "Downloaded daily price data for BRK-B.\n",
      "Downloaded daily price data for JNJ.\n",
      "Downloaded daily price data for JPM.\n",
      "Downloaded daily price data for GOOG.\n",
      "Downloaded daily price data for GOOGL.\n",
      "Downloaded daily price data for XOM.\n",
      "Downloaded daily price data for V.\n",
      "Downloaded daily price data for PG.\n",
      "Downloaded daily price data for BAC.\n",
      "Downloaded daily price data for DIS.\n",
      "Downloaded daily price data for PFE.\n",
      "Downloaded daily price data for T.\n",
      "Downloaded daily price data for CSCO.\n",
      "Downloaded daily price data for VZ.\n",
      "Downloaded daily price data for MA.\n",
      "Downloaded daily price data for CVX.\n",
      "Downloaded daily price data for UNH.\n",
      "Downloaded daily price data for HD.\n",
      "Downloaded daily price data for MRK.\n",
      "Downloaded daily price data for INTC.\n",
      "Downloaded daily price data for KO.\n",
      "Downloaded daily price data for BA.\n",
      "Downloaded daily price data for CMCSA.\n",
      "Downloaded daily price data for WFC.\n",
      "Downloaded daily price data for PEP.\n",
      "Downloaded daily price data for NFLX.\n",
      "Downloaded daily price data for C.\n",
      "Downloaded daily price data for MCD.\n",
      "Downloaded daily price data for WMT.\n",
      "Downloaded daily price data for ABT.\n",
      "Downloaded daily price data for ADBE.\n",
      "Downloaded daily price data for ORCL.\n",
      "Downloaded daily price data for PYPL.\n",
      "Downloaded daily price data for MDT.\n",
      "Downloaded daily price data for HON.\n",
      "Downloaded daily price data for IBM.\n",
      "Downloaded daily price data for PM.\n",
      "Downloaded daily price data for TMO.\n",
      "Downloaded daily price data for UNP.\n",
      "Downloaded daily price data for CRM.\n",
      "Downloaded daily price data for COST.\n",
      "Downloaded daily price data for ACN.\n",
      "Downloaded daily price data for AVGO.\n",
      "Downloaded daily price data for AMGN.\n",
      "Downloaded daily price data for TXN.\n",
      "Downloaded daily price data for LIN.\n",
      "Downloaded daily price data for ^GSPC.\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tickers = [\n",
    "        \"MSFT\", \"AAPL\", \"AMZN\", \"FB\", \"BRK-B\", \"JNJ\", \"JPM\", \"GOOG\", \"GOOGL\", \"XOM\", \n",
    "        \"V\", \"PG\", \"BAC\", \"DIS\" , \"PFE\", \"T\", \"CSCO\", \"VZ\", \"MA\", \"CVX\", \n",
    "        \"UNH\", \"HD\", \"MRK\", \"INTC\", \"KO\", \"BA\", \"CMCSA\", \"WFC\", \"PEP\", \"NFLX\", \n",
    "        \"C\", \"MCD\", \"WMT\", \"ABT\", \"ADBE\", \"ORCL\", \"PYPL\", \"MDT\", \"HON\", \"IBM\", \n",
    "        \"PM\", \"TMO\", \"UNP\", \"CRM\", \"COST\", \"ACN\", \"AVGO\", \"AMGN\", \"TXN\", \"LIN\"\n",
    "    ]\n",
    "    tickers.append(\"^GSPC\")\n",
    "    rundate, start_date, end_date = dt.date(2019, 5, 31), dt.date(2010, 1, 1), dt.date(2019, 5, 31)\n",
    "    df_docs = get_8k_docs(tickers, rundate)\n",
    "    df_financial = get_financial_data(tickers, start_date, end_date)\n",
    "    df = calculate_returns(df_docs, df_financial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
